{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de789ee-53a3-4294-91bd-f7b00dc9f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c26d729-c3ee-4ee8-9a8f-92785b202014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import re\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d590378-da78-4647-a1c3-1109fd98d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    A class representing a node in a tree structure. Each node contains information about its token ID, its parent node,\n",
    "    its children nodes, its depth in the tree, and its cumulative log probability.\n",
    "\n",
    "    Attributes:\n",
    "        token_id (int): The ID of the token associated with this node.\n",
    "        parent_node (Node): The parent node of this node. None if this is the root node.\n",
    "        children (list): A list of child nodes.\n",
    "        depth (int): The depth of this node in the tree.\n",
    "        cum_log_probability (float): The cumulative log probability of this node.\n",
    "        token_sequence (torch.Tensor): A tensor representing the sequence of tokens from the root to this node.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_id: int, parent_node: 'Node', depth: int):\n",
    "        \"\"\"\n",
    "        Initializes a new Node instance.\n",
    "\n",
    "        Args:\n",
    "            token_id (int): The ID of the token associated with this node.\n",
    "            parent_node (Node): The parent node of this node. None if this is the root node.\n",
    "            depth (int): The depth of this node in the tree.\n",
    "        \"\"\"\n",
    "        self.token_id = token_id\n",
    "        self.parent_node = parent_node\n",
    "        self.children = []\n",
    "        self.depth = depth\n",
    "        self.cum_log_probability = None\n",
    "\n",
    "        # Initialize the token_sequence based on the parent node's token_sequence and the current token_id\n",
    "        if depth:\n",
    "            self.token_sequence = torch.cat((parent_node.token_sequence, torch.tensor([self.token_id], dtype=torch.long)))\n",
    "        else:\n",
    "            self.token_sequence = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns a string representation of the node, including its token sequence.\n",
    "\n",
    "        Returns:\n",
    "            str: A string representation of the node.\n",
    "        \"\"\"\n",
    "        return f\"Nodes: {self.token_sequence}, {self.cum_log_probability}\"\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if this node is equal to another node or a tensor.\n",
    "\n",
    "        Args:\n",
    "            other (Node or torch.Tensor): The other node or tensor to compare with.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the nodes are equal, False otherwise.\n",
    "        \"\"\"\n",
    "        if isinstance(other, Node):\n",
    "            return torch.equal(self.token_sequence, other.token_sequence)\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"\"\"\n",
    "        Return the hash based on an immutable attribute. Here, we use the string representation of the token_sequence\n",
    "        because tensors themselves are not hashable and should not be used directly in hash computations if their content\n",
    "        may change.\n",
    "        \"\"\"\n",
    "        return hash(tuple(self.token_sequence.tolist()))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7054b53e-3e47-4037-a42d-4f0e81a979bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SubstringEngine:\n",
    "    def __init__(self, model, tokenizer, mode=False):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = len(sorted(list(self.tokenizer.vocab.keys()),\n",
    "                                        key=lambda x: len(x), reverse=True)[0])\n",
    "        self.mode = True\n",
    "\n",
    "    def _expand_tree(self, parent: Node,\n",
    "                    tokenized_candidates: List[torch.Tensor],\n",
    "                    height: int,\n",
    "                    position: int = 0,\n",
    "                    special_ids: List[int] = []) -> Node:\n",
    "        \"\"\"\n",
    "        Expands the tree from a given parent node by adding child nodes based on the tokenized context.\n",
    "    \n",
    "        Args:\n",
    "            parent (Node): The parent node from which to expand the tree.\n",
    "            tokenized_candidates (List[torch.Tensor]): The tokenized context for the prompt.\n",
    "            height (int): The height of the tree to expand to.\n",
    "            position (int, optional): The current position in the tokenized context. Defaults to 0.\n",
    "            special_ids (List[int], optional): A list of special token IDs to exclude from the tree. Defaults to an empty list.\n",
    "    \n",
    "        Returns:\n",
    "            Node: The parent node with its children expanded.\n",
    "        \"\"\"\n",
    "        # Iterate over each context in the tokenized context\n",
    "        for candidate in tokenized_candidates:\n",
    "            # Get the token at the current position\n",
    "            if position < len(candidate):\n",
    "                token = candidate[position].item()\n",
    "                # Check if the token is not a special token and if it's not already a child of the parent\n",
    "                if (torch.equal(candidate[:position], parent.token_sequence) and \n",
    "                    all(token != child.token_id for child in parent.children) and\n",
    "                    token not in special_ids):\n",
    "                    \n",
    "                    # Create a new node with the current token and add it as a child to the parent\n",
    "                    new_node = Node(token, parent, parent.depth + 1)\n",
    "                    parent.children.append(new_node)\n",
    "        \n",
    "                    # Recursively expand the tree if the current position is less than the height\n",
    "                    if new_node.depth < height:\n",
    "                        self._expand_tree(new_node, tokenized_candidates, height, position + 1, special_ids)\n",
    "        # Return the parent node with its children expanded\n",
    "        return parent\n",
    "\n",
    "    \n",
    "    def _build_tree(self, tokenized_context: List[torch.Tensor]) -> Tuple[Node, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Builds the entire tree for a given prompt using the tokenized context.\n",
    "    \n",
    "        Args:\n",
    "            promt (str): The prompt for which the tree is being built.\n",
    "            tokenized_context (List[torch.Tensor]): The tokenized context for the prompt.    \n",
    "        Returns:\n",
    "            Tuple[Node, torch.Tensor]: The root node of the tree and the tokenized prompt.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = time.time()\n",
    "        \n",
    "        # Initialize the root node and tokenize the prompt\n",
    "        root = Node(-1, None, 0)\n",
    "        # Expand the tree from the root node to the specified height, excluding special tokens\n",
    "        root = self._expand_tree(root,\n",
    "                           tokenized_context,\n",
    "                           len(tokenized_context[0]),\n",
    "                           special_ids = self.tokenizer.all_special_ids)\n",
    "        # Set the cumulative log probability of the root node to 0\n",
    "        root.cum_log_probability = 0\n",
    "        \n",
    "        if self.mode:\n",
    "            print(f\"build tree for first tokens: {time.time() - s}\")\n",
    "        \n",
    "        # Return the root node and the tokenized prompt\n",
    "        return root\n",
    "\n",
    "\n",
    "    def _candidate_sequences(self, context, max_token_length, prompt=''):\n",
    "        \"\"\"\n",
    "        Generates a set of candidate sequences from the given context by considering all\n",
    "        possible substrings within a specified length limit.\n",
    "        \n",
    "        These candidates are then prefixed with the provided prompt to form complete sequences.\n",
    "    \n",
    "        Args:\n",
    "            context (str): The input context from which to generate candidate sequences.\n",
    "            max_token_length (int): The maximum length of a candidate sequence in terms of tokens.\n",
    "            prompt (str, optional): A prefix to be added to each candidate sequence. Defaults to an empty string.\n",
    "    \n",
    "        Returns:\n",
    "            list: A list of candidate sequences, each starting with the provided prompt.\n",
    "    \n",
    "        The function first calculates the restriction based on the length of the context and the maximum token length.\n",
    "        It then iterates over the text to generate all possible substrings within this restriction.\n",
    "        These substrings are added to a set to ensure uniqueness. The set is then sorted for reproducibility,\n",
    "        and each candidate is prefixed with the prompt to form complete\n",
    "        sequences. These sequences are returned as a list.\n",
    "        \"\"\"\n",
    "        s = time.time()\n",
    "        \n",
    "        # Calculate the restriction based on the length of the text and the maximum token length\n",
    "        restriction = min(len(context) + 1, max_token_length)\n",
    "        # Initialize an empty set to store unique substring candidates\n",
    "        substring_candidates = set()\n",
    "        # Iterate over the text to generate all possible substrings within the restriction\n",
    "        for i in range(len(context)):\n",
    "            for j in range(i+1, restriction):\n",
    "                substring_candidates.add(context[i:j])\n",
    "        \n",
    "        # Sort the set of substring candidates for reproducibility\n",
    "        substring_candidates = sorted(substring_candidates)\n",
    "        # Prefix each candidate with the prompt to form complete sequences\n",
    "        sequences = [prompt + candidate for candidate in substring_candidates]\n",
    "\n",
    "        if self.mode:\n",
    "            print(f\"get candidates: last 2 tokens + all substring candidates {time.time() - s}\")\n",
    "            \n",
    "        return sequences\n",
    "\n",
    "    \n",
    "    def _compute_logprob(self, common_part, nodes):\n",
    "        \"\"\"\n",
    "        Computes the cumulative log probabilities for each node in the tree structure,\n",
    "        given a common part of the text (user prompt wihtout last 2 tokens) and a list of nodes.\n",
    "    \n",
    "        This function is crucial for evaluating the likelihood of each candidate sequence generated from the context text.\n",
    "        It does so by leveraging the transformer model to predict the next token in the sequence and then calculating\n",
    "        the log probability of each token. \n",
    "        \n",
    "        Args:\n",
    "            common_part (str): A common part of the text (user prompt wihtout last 2 tokens) that is shared by all\n",
    "                               nodes in the tree. This is used to ensure that the model's predictions are relevant\n",
    "                               to the context of the input prompt.\n",
    "            nodes (List[Node]): A list of nodes for which the cumulative log probabilities are to be computed.\n",
    "    \n",
    "        Returns:\n",
    "            None: The function modifies the nodes in-place, updating their cumulative log probabilities.\n",
    "    \n",
    "        The function begins by initializing an empty list for the input batch and two empty lists for mapping nodes\n",
    "        to their corresponding log probabilities and input sequences. It then iterates over each node, checking if\n",
    "        its cumulative log probability has been set. If not, it constructs the input sequence for the model by \n",
    "        concatenating the common part of the text with the token sequence of the node. This input sequence is then\n",
    "        added to the input batch and the node is mapped to its corresponding log probability.\n",
    "    \n",
    "        Once all nodes have been processed, the function tokenizes the input batch using the tokenizer and feeds\n",
    "        it into the model to get the logits. The log probabilities are then calculated using the log_softmax function.\n",
    "    \n",
    "        Finally, the function iterates over the nodes again, this time updating their cumulative log probabilities\n",
    "        based on the log probabilities of their tokens and the cumulative log probabilities of their parent nodes.\n",
    "        \n",
    "        This process ensures that each node's cumulative log probability reflects the likelihood\n",
    "        of the sequence of tokens leading up to it.\n",
    "        \"\"\"\n",
    "        if self.mode:\n",
    "            print(\"log prob nodes: \")\n",
    "            print(len(nodes))\n",
    "            for node in nodes:\n",
    "                print(self.tokenizer.decode(node.token_sequence))\n",
    "            print()\n",
    "\n",
    "        s = time.time()\n",
    "        \n",
    "        # Calculate the number of tokens in the common part of the text\n",
    "        skip_logits = len(self.tokenizer.encode(common_part))\n",
    "        # Initialize dicts for the (key: input text, value: token) and log probabilities mapping (key: node, value: prob)\n",
    "        node_mapping = {}\n",
    "        log_probs_mapping = {}\n",
    "    \n",
    "        # Iterate over each node\n",
    "        for node in nodes:\n",
    "            # Check if the node's cumulative log probability has been set\n",
    "            if node.cum_log_probability is None:\n",
    "                # Construct the input sequence for the model\n",
    "                # we get tokens of the parent, since we need only\n",
    "                # them for getting log probability for current considered node token\n",
    "                inp = common_part + self.tokenizer.decode(node.parent_node.token_sequence)\n",
    "                # Map the node to its input sequence\n",
    "                node_mapping.setdefault(inp, []).append(node)\n",
    "    \n",
    "        # If there are no nodes to process, return\n",
    "        if not node_mapping:\n",
    "            return\n",
    "    \n",
    "        # Tokenize the input batch\n",
    "        tokenized_model_input = self.tokenizer(list(node_mapping.keys()),\n",
    "                                           return_tensors=\"pt\",\n",
    "                                           padding=True,\n",
    "                                           add_special_tokens=True)\n",
    "        \n",
    "        # Feed the tokenized input into the model to get the logits\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized_model_input)\n",
    "            logits = outputs.logits[:, skip_logits-1:, :]\n",
    "            # Calculate the log probabilities\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    \n",
    "        # Iterate over the nodes again to update their cumulative log probabilities\n",
    "        for idx, inp in enumerate(list(node_mapping.keys())):\n",
    "            for node in node_mapping[inp]:\n",
    "                # Get the tokens for the current node\n",
    "                tokens = tokenized_model_input['input_ids'][idx, skip_logits-1:]\n",
    "                \n",
    "                # Calculate the number of tokens before padding\n",
    "                first_padding = torch.sum(tokens != self.tokenizer.pad_token_id).item()\n",
    "        \n",
    "                # It is possible, that parent node cum_log_probability is not calculated yet\n",
    "                # Thus, if it is a such situation, we will calculate log_probability for parent nodes also\n",
    "                \n",
    "                # Initialize lists for the parent nodes and their log probabilities\n",
    "                parents_log_probs = []\n",
    "                parents_sequence_without_logprob = []\n",
    "                # Iterate over the parent nodes\n",
    "                parent_tmp = node.parent_node\n",
    "                i = 2\n",
    "                while parent_tmp.cum_log_probability is None:\n",
    "                    parents_sequence_without_logprob.append(parent_tmp)\n",
    "                    parents_log_probs.append(log_probs[idx, first_padding-i, tokens[first_padding-i+1]])\n",
    "                    i += 1\n",
    "                    parent_tmp = parent_tmp.parent_node\n",
    "                \n",
    "                # Calculate the cumulative log probability for each parent node\n",
    "                number_of_parents_without_logbrob = len(parents_sequence_without_logprob)\n",
    "                for n_id in range(number_of_parents_without_logbrob - 1, -1, -1):\n",
    "                    if n_id == number_of_parents_without_logbrob - 1:\n",
    "                        parents_sequence_without_logprob[n_id].cum_log_probability = (parents_log_probs[n_id] + \n",
    "                                                                                 parents_sequence_without_logprob[n_id].parent_node.cum_log_probability)\n",
    "                    else:\n",
    "                        parents_sequence_without_logprob[n_id].cum_log_probability = (parents_log_probs[n_id] + \n",
    "                                                              parents_sequence_without_logprob[n_id].parent_node.cum_log_probability)\n",
    "                \n",
    "                # Update the node's cumulative log probability\n",
    "                log_probs_mapping[node] = node.parent_node.cum_log_probability\n",
    "                node.cum_log_probability = log_probs_mapping[node] + log_probs[idx, first_padding-1, node.token_id]\n",
    "\n",
    "        if self.mode:\n",
    "            print(f\"compute log_probs call {time.time() - s}\")\n",
    "        \n",
    "    \n",
    "    def _get_topk_nodes(self, nodes, k):\n",
    "        \"\"\"\n",
    "        Selects the top `k` nodes from a given list of nodes based on their cumulative log probabilities, normalized by their depth.\n",
    "    \n",
    "        This function is used to prune the tree and focus on the most promising candidates for further evaluation or output.\n",
    "        By normalizing the cumulative log probabilities by the depth of each node,\n",
    "        it ensures that nodes deeper in the tree are not overly favored simply because they are longer.\n",
    "    \n",
    "        Args:\n",
    "            nodes (List[Node]): A list of nodes from which to select the top `k` nodes.\n",
    "            k (int): The number of top nodes to select.\n",
    "    \n",
    "        Returns:\n",
    "            List[Node]: A list of the top `k` nodes, sorted by their normalized cumulative log probabilities.\n",
    "    \n",
    "        The function begins by calculating the normalized scores for each node.\n",
    "        This is done by dividing the cumulative log probability of each node by its depth.\n",
    "        The scores are then converted into a tensor and the indices of the top `k` scores \n",
    "        are determined using the `torch.topk` function. These indices are used to select \n",
    "        the corresponding nodes from the original list.\n",
    "    \n",
    "        The selected nodes are returned as a list, which can then be used for further processing\n",
    "        or output. This function is particularly useful in scenarios where the tree is large and \n",
    "        contains many nodes, allowing the script to efficiently focus on the most likely candidates.\n",
    "        \"\"\"\n",
    "        s = time.time()\n",
    "        # Calculate the normalized scores for each node\n",
    "        scores = torch.tensor([node.cum_log_probability/node.depth for node in nodes])\n",
    "        # Determine the indices of the top k scores\n",
    "        top_k_indices = torch.topk(scores, k=k, largest=True, sorted=True).indices\n",
    "        \n",
    "        if self.mode:\n",
    "            print(f\"get top k nodes call: {time.time() - s}\")\n",
    "        # Select the top k nodes using the indices\n",
    "        return [nodes[i] for i in top_k_indices]\n",
    "\n",
    "\n",
    "    def _candidate_sequences_exp(self, context, chosen_options, max_candidate_length, prompt=''):\n",
    "        \"\"\"\n",
    "        Generates a set of candidate sequences from the given context,\n",
    "        with each candidate starting with one of the chosen options.\n",
    "    \n",
    "        This function is useful for scenarios where the context or the prompt suggests\n",
    "        specific starting points for the sequences, allowing for more targeted generation.\n",
    "        \n",
    "        It iterates over the text to generate all possible substrings within a specified length\n",
    "        limit and checks if each candidate starts with one of the chosen options. Only those \n",
    "        candidates that meet this condition are added to the set of substring candidates.\n",
    "    \n",
    "        Args:\n",
    "            context (str): The input context from which to generate candidate sequences.\n",
    "            chosen_options (List[str]): A list of options that each candidate sequence must start with.\n",
    "            max_candidate_length (int): The maximum length of a candidate sequence in terms of tokens.\n",
    "            prompt (str, optional): A prefix to be added to each candidate sequence. Defaults to an empty string.\n",
    "    \n",
    "        Returns:\n",
    "            list: A list of candidate sequences, each starting with one of the chosen options and prefixed with the provided prompt.\n",
    "    \n",
    "        The function first calculates the restriction based on the length of the text and the maximum candidate length. It then iterates over the text to generate all possible substrings within this restriction. For each substring, it checks if the substring starts with one of the chosen options. If so, the substring is added to the set of substring candidates. The set is then sorted for reproducibility, and each candidate is prefixed with the prompt to form complete sequences. These sequences are returned as a list.\n",
    "        \"\"\"\n",
    "        s = time.time()\n",
    "        # Calculate the restriction based on the length of the text and the maximum candidate length\n",
    "        restriction = min(len(context) + 1, max_candidate_length)\n",
    "        # Initialize an empty set to store unique substring candidates\n",
    "        substring_candidates = set()\n",
    "        # Iterate over the text to generate all possible substrings within the restriction\n",
    "        for i in range(len(context)):\n",
    "            for j in range(i+1, restriction):\n",
    "                candidate = prompt + context[i:j]\n",
    "                tokenized_candidate = self.tokenizer.encode(candidate,\n",
    "                                                       return_tensors=\"pt\",\n",
    "                                                      add_special_tokens=False)[0]\n",
    "                # Check if the candidate starts with one of the chosen options\n",
    "                if any(torch.equal(tokenized_candidate[:len(option)], option) for option in chosen_options):\n",
    "                    substring_candidates.add(candidate)\n",
    "        \n",
    "        # Sort the set of substring candidates for reproducibility\n",
    "        sequences = sorted(list(substring_candidates))\n",
    "\n",
    "        if self.mode:\n",
    "            print(f\"get expanded candidates starts with top k first tokens: {time.time() - s}\")\n",
    "        return sequences\n",
    "\n",
    "    \n",
    "    def _get_nodes_seq_before_branch(self, node):\n",
    "        \"\"\"\n",
    "        Traverses the tree structure from a given node and returns the node that is just before a branching point.\n",
    "    \n",
    "        Args:\n",
    "            node (Node): The starting node from which to traverse the tree.\n",
    "    \n",
    "        Returns:\n",
    "            Node: The node that is just before a branching point in the tree.\n",
    "    \n",
    "        The function begins by entering a loop that continues until it finds a node with more than one child.\n",
    "        It starts with the given node and checks its children. If the node has exactly one child, the function \n",
    "        moves to that child and continues the process. This ensures that the function traverses down the tree \n",
    "        until it reaches a node that is about to branch into multiple paths.\n",
    "    \n",
    "        Once the branching point is found, the function breaks out of the loop and returns the node that led to \n",
    "        this branching. This node is the one just before the branching point, and it can be used for further \n",
    "        processing or analysis.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            children = node.children\n",
    "            # If the node has exactly one child, move to that child\n",
    "            if len(children) == 1:\n",
    "                node = children[0]\n",
    "            else:\n",
    "                # If the node has more than one child, it's a branching point\n",
    "                break\n",
    "        # Return the node just before the branching point\n",
    "        return node\n",
    "\n",
    "\n",
    "    def _iteration(self, working_list, common_part, k):\n",
    "        \"\"\"\n",
    "        Performs an iteration of the sequence generation process by computing the cumulative log probabilities\n",
    "        of the nodes in the working list and selecting the top `k` nodes.\n",
    "    \n",
    "        Args:\n",
    "            working_list (List[Node]): The list of nodes for which the cumulative log probabilities are to be computed\n",
    "                                       and from which the top `k` nodes are to be selected.\n",
    "            common_part (str): A common part of the text that is shared by all nodes in the tree. This is used to ensure\n",
    "                               that the model's predictions are relevant to the context of the input text.\n",
    "            k (int): The number of top nodes to select.\n",
    "    \n",
    "        Returns:\n",
    "            List[Node]: The top `k` nodes from the working list, sorted by their normalized cumulative log probabilities.\n",
    "        \"\"\"\n",
    "        # Add children to wotking list for every node in it\n",
    "        working_list = self._update_working_list_with_children(working_list)\n",
    "        \n",
    "        # Compute the cumulative log probabilities for each node in the working list\n",
    "        self._compute_logprob(common_part, working_list)\n",
    "        # Select the top k nodes from the working list based on their cumulative log probabilities\n",
    "        return self._get_topk_nodes(working_list, k)\n",
    "\n",
    "\n",
    "    def _update_working_list_with_children(self, working_list):\n",
    "        \"\"\"\n",
    "        Updates the working list of nodes by adding the children of each node in the list,\n",
    "        specifically those that are just before a branching point in the tree.\n",
    "    \n",
    "        Args:\n",
    "            working_list (List[Node]): The current working list of nodes to be updated.\n",
    "    \n",
    "        Returns:\n",
    "            List[Node]: The updated working list of nodes, including the children of each node in the original list.\n",
    "        \"\"\"\n",
    "        s = time.time()\n",
    "        for node in working_list:\n",
    "            for c in node.children:\n",
    "                candidate_to_add = self._get_nodes_seq_before_branch(c)\n",
    "                if candidate_to_add not in working_list:\n",
    "                    working_list.append(candidate_to_add)\n",
    "\n",
    "        if self.mode:\n",
    "            print(f\"add children sequences before found branch into working list: {time.time() - s}\")\n",
    "        return working_list\n",
    "\n",
    "    \n",
    "    def substring(self, prompt, context, k, max_substring_length, return_full_text=False):\n",
    "        \"\"\"\n",
    "        Generates and evaluates candidate sequences based on a given prompt and context, selecting the top `k` nodes.\n",
    "    \n",
    "        Args:\n",
    "            prompt (str): The prompt for which the tree is being built and from which the last part and the common part are extracted.\n",
    "            context (str): The context from which candidate sequences are generated.\n",
    "            k (int): The number of top nodes to select.\n",
    "            max_substring_length (int): The maximum length of a candidate sequence in terms of tokens.\n",
    "    \n",
    "        Returns:\n",
    "            Tuple[List[Node], Node]: The top `k` nodes from the working list and the initial root node of the tree.\n",
    "        \"\"\"\n",
    "        # Tokenize the prompt and extract the last part and the common part\n",
    "        tokenized_prompt = self.tokenizer(prompt,\n",
    "                                    return_tensors=\"pt\",\n",
    "                                    padding=True,\n",
    "                                    add_special_tokens=False)['input_ids']\n",
    "        last_part = self.tokenizer.decode(tokenized_prompt[0, -2:])\n",
    "        common_part = self.tokenizer.decode(tokenized_prompt[0, :-2])\n",
    "\n",
    "        if len(context) == 1:\n",
    "            if return_full_text:\n",
    "                return common_part + context\n",
    "            return context\n",
    "            \n",
    "        \n",
    "        # Generate candidate sequences from the context\n",
    "        substring_candidates = self._candidate_sequences(context, self.max_token_len, last_part)\n",
    "        print(f\"substring candidates: {substring_candidates}\")\n",
    "        tokenized_s_cand = self.tokenizer(substring_candidates,\n",
    "                                     return_tensors=\"pt\",\n",
    "                                     padding=True,\n",
    "                                     add_special_tokens=False)['input_ids']\n",
    "    \n",
    "        # Build the tree structure from the tokenized candidate sequences\n",
    "        initial_root = self._build_tree(tokenized_s_cand)                \n",
    "    \n",
    "        # Expand the tree from the root node\n",
    "        node_before_branch = self._get_nodes_seq_before_branch(initial_root)\n",
    "        first_branch_children = node_before_branch.children\n",
    "\n",
    "        if not first_branch_children:\n",
    "            first_branch_children = [node_before_branch]\n",
    "\n",
    "        # Last token of the prompt can be changed\n",
    "        # Therefore, we have to capture not tokens before first branch, but all its children after branch   \n",
    "        \n",
    "        working_list = []\n",
    "        for node in first_branch_children:\n",
    "            wl_len = len(working_list)\n",
    "            \n",
    "            for c in node.children:\n",
    "                candidate_to_add = self._get_nodes_seq_before_branch(c)\n",
    "                if candidate_to_add not in working_list:\n",
    "                    working_list.append(candidate_to_add)  \n",
    "                    \n",
    "            if wl_len == len(working_list):\n",
    "                working_list.append(node)\n",
    "                    \n",
    "\n",
    "        self._compute_logprob(common_part, working_list)\n",
    "        working_list = self._get_topk_nodes(working_list, k)\n",
    "    \n",
    "        # Generate expanded candidate sequences based on the chosen candidates\n",
    "        chosen_candidates = list(map(lambda x: x.token_sequence, working_list))\n",
    "\n",
    "        if self.mode:\n",
    "            print(\"chosen_candidates for explansion: \")\n",
    "            for c in chosen_candidates:\n",
    "                print(self.tokenizer.decode(c))\n",
    "            print()\n",
    "        \n",
    "        expanded_candidates = self._candidate_sequences_exp(context, chosen_candidates, max_substring_length, last_part)\n",
    "        tokenized_expanded_candidates = self.tokenizer(expanded_candidates,\n",
    "                                                  return_tensors=\"pt\",\n",
    "                                                  padding=True,\n",
    "                                                  add_special_tokens=False)['input_ids']\n",
    "        \n",
    "        # Update the working list with children nodes\n",
    "        working_list[0].parent_node.children = working_list\n",
    "\n",
    "    \n",
    "        # Expand the tree with the expanded candidate sequences\n",
    "        for node in working_list:\n",
    "            self._expand_tree(node,\n",
    "                        tokenized_expanded_candidates,\n",
    "                        len(tokenized_expanded_candidates[0]),\n",
    "                        position = node.depth,\n",
    "                        special_ids=self.tokenizer.all_special_ids)\n",
    "    \n",
    "        # Iteratively refine the set of candidate sequences based on their likelihood\n",
    "        while True:\n",
    "            prev_w_l = deepcopy(working_list)\n",
    "            working_list = self._iteration(working_list, common_part, k)\n",
    "            if prev_w_l == working_list:\n",
    "                break\n",
    "    \n",
    "        res_text = []\n",
    "        for node in working_list:\n",
    "            substirng_choice = self.tokenizer.decode(node.token_sequence)\n",
    "            \n",
    "            if return_full_text:\n",
    "                res_text.append(common_part + substirng_choice)\n",
    "            else:\n",
    "                res_text.append(substirng_choice[len(last_part):])\n",
    "        \n",
    "        return res_text, working_list, initial_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da1ed9f-a16d-4918-8b23-3118d069a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria\n",
    "\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, eos_sequence):\n",
    "        self.eos_sequence = eos_sequence\n",
    "\n",
    "    def __call__(self,\n",
    "                 input_ids: torch.LongTensor,\n",
    "                 scores: torch.FloatTensor,\n",
    "                 **kwargs) -> bool:\n",
    "        \n",
    "        # Check each batch item if the sequence ends with the specified eos_sequence\n",
    "        last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "        # Check if all elements in eos_sequence match for any item in the batch\n",
    "        return self.eos_sequence in last_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f2f130-2976-4f4d-8a15-25d94c3170d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidanceBeta:\n",
    "    \"\"\"\n",
    "    Class for generating guidance using a pretrained language model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Pretrained model identifier from Hugging Face model hub.\n",
    "        mode (bool): Mode for the guidance generation (whether to print log messages or not).\n",
    "        model_kwargs (dict): Additional keyword arguments to pass to the model initialization.\n",
    "        tokenizer_kwargs (dict): Additional keyword arguments to pass to the tokenizer initialization.\n",
    "\n",
    "    Attributes:\n",
    "        model (AutoModelForCausalLM): Pretrained model for generating guidance.\n",
    "        tokenizer (AutoTokenizer): Tokenizer for tokenizing inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name,\n",
    "                 mode=True,\n",
    "                 model_kwargs=None,\n",
    "                 tokenizer_kwargs=None):\n",
    "        \n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "        if tokenizer_kwargs is None:\n",
    "            tokenizer_kwargs = {}\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                          **model_kwargs)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                                      **tokenizer_kwargs)\n",
    "\n",
    "        if not self.tokenizer.pad_token:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.special_tokens = torch.tensor(self.tokenizer.all_special_ids)\n",
    "        self.mode = mode\n",
    "    \n",
    "    def _tokenize_inputs(self, texts, choices_list):\n",
    "        inputs = []\n",
    "        for text, choices in zip(texts, choices_list):\n",
    "            for choice in choices:\n",
    "                inputs.append(f\"{text}{choice}\")\n",
    "\n",
    "        tokenized_inputs = self.tokenizer(inputs, return_tensors=\"pt\", padding=True, add_special_tokens=True)\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n",
    "\n",
    "    def select(self, input_batches, choices_list, return_full_text=False):\n",
    "        \"\"\"\n",
    "        Select the most appropriate choice for each text.\n",
    "\n",
    "        Args:\n",
    "            texts (list): List of input texts or one stirng with input.\n",
    "            choices_list (list of lists): List of lists of choices corresponding to each text.\n",
    "\n",
    "        Returns:\n",
    "            list: List of selected choices.\n",
    "        \"\"\"\n",
    "        if isinstance(input_batches, str):\n",
    "            input_batches = [input_batches]\n",
    "        \n",
    "        tokenized_inputs = self._tokenize_inputs(input_batches, choices_list)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized_inputs)            \n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Apply log softmax to convert logits to probabilities\n",
    "            probabilities = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        returned_text = []\n",
    "        \n",
    "        logits_slice_begin = 0\n",
    "\n",
    "        \n",
    "        for text_idx, pair in enumerate(zip(input_batches, choices_list)):\n",
    "            text, choices = pair\n",
    "            \n",
    "            # Number of tokens to skip, since they are common in given text\n",
    "            skip_logits = len(self.tokenizer.encode(text)) - 1\n",
    "\n",
    "            # Get the number of different variants to select\n",
    "            number_of_options = len(choices)\n",
    "\n",
    "            logits_slice_end = logits_slice_begin + number_of_options\n",
    "\n",
    "            # Extracting logits for specific tokens\n",
    "            probabilities_slice = probabilities[logits_slice_begin:logits_slice_end, skip_logits-1:-1, :]\n",
    "\n",
    "            # Getting indices of tokens from input_ids\n",
    "            input_ids_slice = tokenized_inputs['input_ids'][logits_slice_begin:logits_slice_end, skip_logits:]\n",
    "\n",
    "            # Create a mask tensor in order not to count probability of special tokens\n",
    "            mask = torch.where(torch.isin(input_ids_slice, self.special_tokens), torch.tensor(0), torch.tensor(1))\n",
    "\n",
    "            # Adding a dimension to input_ids_slice\n",
    "            input_ids_slice_expanded = input_ids_slice.unsqueeze(-1)\n",
    "\n",
    "            # Gathering logits for the specified tokens\n",
    "            selected_probabilities = probabilities_slice.gather(dim=-1, index=input_ids_slice_expanded).squeeze(-1)\n",
    "            selected_probabilities_masked = selected_probabilities * mask\n",
    "\n",
    "            # Getting log probabilities of\n",
    "            log_probs = torch.mean(selected_probabilities_masked, dim=-1) \n",
    "            choice_idx = torch.argmax(log_probs).item()\n",
    "\n",
    "            if return_full_text:\n",
    "                returned_text.append(f\"{text}{choices[choice_idx]}\")\n",
    "            else:\n",
    "                returned_text.append(f\"{choices[choice_idx]}\")\n",
    "            \n",
    "            logits_slice_begin = logits_slice_end\n",
    "\n",
    "        return returned_text\n",
    "\n",
    "    \n",
    "    def gen(self,\n",
    "            input_batches,\n",
    "            stop_keywords=None,\n",
    "            return_full_text=False,\n",
    "            **kwargs):\n",
    "        \"\"\"\n",
    "        Generate text based on input batches.\n",
    "\n",
    "        Args:\n",
    "            input_batches (str or list of str): Input text or list of input texts.\n",
    "            stop_token (str, optional): Token at which to stop generation.\n",
    "            return_full_text (bool, optional): Whether to return the full generated text.\n",
    "            **kwargs: Additional keyword arguments for the generation method.\n",
    "                details on kwargs: \n",
    "                https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    \n",
    "        Returns:\n",
    "            list of str: List of generated texts.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Ensure input_batches is a list\n",
    "        if isinstance(input_batches, str):\n",
    "            input_batches = [input_batches]\n",
    "    \n",
    "        # Tokenize all input batches at once\n",
    "        inputs = self.tokenizer(input_batches, return_tensors=\"pt\", padding=True, add_special_tokens=True)\n",
    "\n",
    "        inputs['input_ids'] = inputs['input_ids'].to('cuda')\n",
    "\n",
    "        # Convert the stop_token to its token ID if provided\n",
    "        if stop_keywords:\n",
    "            keyword_token_ids = self.tokenizer.encode(stop_keywords, add_special_tokens=False)\n",
    "            stopping_criteria = EosListStoppingCriteria(eos_sequence=keyword_token_ids)\n",
    "            kwargs['stopping_criteria'] = [stopping_criteria]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            # Generate text for all input batches in a single call\n",
    "            generated_text = self.model.generate(inputs['input_ids'], **kwargs)\n",
    "            # Decode the generated text\n",
    "            generated_texts = self.tokenizer.batch_decode(generated_text, skip_special_tokens=True)\n",
    "\n",
    "        if not return_full_text:\n",
    "            return [result[len(batch):] for result, batch in zip(generated_texts, input_batches)]\n",
    "        return generated_texts\n",
    "    \n",
    "    def substring(self, input_text, context:str, k=1, max_substring_length=35):\n",
    "        substring_engine = SubstringEngine(self.model, self.tokenizer, mode=self.mode)\n",
    "        # return res_text, working_list, initial_root\n",
    "        result = substring_engine.substring(input_text, context, k, max_substring_length)\n",
    "        \n",
    "        return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25ed5651-9aa4-48a4-8f40-1475e05f78e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "access_token = \"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\"\n",
    "model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "model_kwargs = {\n",
    "    'token': access_token,\n",
    "    'device_map': 'auto',\n",
    "    'attn_implementation': 'flash_attention_2',\n",
    "    'torch_dtype': torch.bfloat16\n",
    "}\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    'token': access_token,\n",
    "    'device_map': 'auto'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c251854-47ec-4f73-aee1-1f9567891f79",
   "metadata": {},
   "source": [
    "Сейчас на сабстринге включены принты, чтобы следить за временем каждого этапа алогоритма.\n",
    "Если хочешь выключить, то передай в инициализацию GuidanceBeta аргумент `mode=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52680195-8166-4d7c-95ec-c22af762e9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3813fa5abea4590886eaf939ead414e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "guidance_system = GuidanceBeta(model_name,\n",
    "                               model_kwargs=model_kwargs,\n",
    "                               tokenizer_kwargs=tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0120e96b-f9ef-419f-9215-b493282a5ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get candidates: last 2 tokens + all substring candidates 2.1219253540039062e-05\n",
      "substring candidates: ['? L', '? N', '? NL']\n",
      "build tree for first tokens: 0.00089263916015625\n",
      "log prob nodes: \n",
      "2\n",
      "? L\n",
      "? NL\n",
      "\n",
      "compute log_probs call 0.07478117942810059\n",
      "get top k nodes call: 6.198883056640625e-05\n",
      "chosen_candidates for explansion: \n",
      "? L\n",
      "? NL\n",
      "\n",
      "get expanded candidates starts with top k first tokens: 0.0004696846008300781\n",
      "add children sequences before found branch into working list: 1.1920928955078125e-06\n",
      "log prob nodes: \n",
      "2\n",
      "? L\n",
      "? NL\n",
      "\n",
      "get top k nodes call: 4.172325134277344e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11781787872314453"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"How many parameters does BLOOM have? \"\n",
    "context = \"NL\"\n",
    "\n",
    "s = time.time()\n",
    "res = guidance_system.substring(prompt, context, 2)\n",
    "time.time() - s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b228c29a-4d7c-4772-bf6c-6befadc427bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L', 'NL']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c327edf-56cf-4936-855f-f9e25e457bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: tensor([1577,  405]), -9.566487312316895\n",
      "? N\n",
      "Nodes: tensor([1577,  365]), -9.628987312316895\n",
      "? L\n",
      "Nodes: tensor([ 1577,   405, 29931]), -16.361896514892578\n",
      "? NL\n"
     ]
    }
   ],
   "source": [
    "for node in res[1]:\n",
    "    print(node)\n",
    "    print(guidance_system.tokenizer.decode(node.token_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c29f88d-257a-454f-aad5-6b0a8a8c66cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get candidates: last 2 tokens + all substring candidates 0.0001647472381591797\n",
      "build tree for first tokens: 0.063446044921875\n",
      "add children sequences before found branch into working list: 0.0027408599853515625\n",
      "log prob nodes: \n",
      "88\n",
      "so  b\n",
      "so  ba\n",
      "so  bad\n",
      "so  d\n",
      "so  da\n",
      "so  day!\n",
      "so ! \n",
      "so ! b\n",
      "so ! ba\n",
      "so ! bad\n",
      "so ad \n",
      "so ad d\n",
      "so ay!\n",
      "so bad \n",
      "so bad d\n",
      "so d \n",
      "so d d\n",
      "so d da\n",
      "so d day!\n",
      "so day!\n",
      "so goo\n",
      "so good \n",
      "so good d\n",
      "so good da\n",
      "so good day!\n",
      "so oo\n",
      "so ood\n",
      "so od \n",
      "so od d\n",
      "so od da\n",
      "so od day!\n",
      "so y!\n",
      "so  bad \n",
      "so  bad d\n",
      "so  day! \n",
      "so  day! b\n",
      "so  day! ba\n",
      "so  day! bad\n",
      "so ! bad \n",
      "so ! bad d\n",
      "so ay! \n",
      "so ay! b\n",
      "so ay! ba\n",
      "so ay! bad\n",
      "so d day! \n",
      "so d day! b\n",
      "so d day! ba\n",
      "so d day! bad\n",
      "so day! \n",
      "so day! b\n",
      "so day! ba\n",
      "so day! bad\n",
      "so good day! \n",
      "so good day! b\n",
      "so good day! ba\n",
      "so good day! bad\n",
      "so ood \n",
      "so ood d\n",
      "so ood da\n",
      "so ood day!\n",
      "so od day! \n",
      "so od day! b\n",
      "so od day! ba\n",
      "so od day! bad\n",
      "so y! \n",
      "so y! b\n",
      "so y! ba\n",
      "so y! bad\n",
      "so  day! bad \n",
      "so  day! bad d\n",
      "so ay! bad \n",
      "so ay! bad d\n",
      "so d day! bad \n",
      "so d day! bad d\n",
      "so day! bad \n",
      "so day! bad d\n",
      "so good day! bad \n",
      "so good day! bad d\n",
      "so ood day! \n",
      "so ood day! b\n",
      "so ood day! ba\n",
      "so ood day! bad\n",
      "so od day! bad \n",
      "so od day! bad d\n",
      "so y! bad \n",
      "so y! bad d\n",
      "so ood day! bad \n",
      "so ood day! bad d\n",
      "\n",
      "compute log_probs call 0.16489768028259277\n",
      "get top k nodes call: 0.00041556358337402344\n",
      "chosen_candidates for explansion: \n",
      "so good day! \n",
      "so ood day! \n",
      "so good day!\n",
      "so  day! \n",
      "\n",
      "get expanded candidates starts with top k first tokens: 0.03369450569152832\n",
      "add children sequences before found branch into working list: 0.00012445449829101562\n",
      "log prob nodes: \n",
      "21\n",
      "so good day! \n",
      "so ood day! \n",
      "so good day!\n",
      "so  day! \n",
      "so good day! b\n",
      "so good day! ba\n",
      "so good day! bad\n",
      "so good day! bad \n",
      "so good day! bad d\n",
      "so good day! bad da\n",
      "so good day! bad day,\n",
      "so good day! bad day, \n",
      "so good day! bad day, I\n",
      "so good day! bad day, I \n",
      "so good day! bad day, I h\n",
      "so good day! bad day, I ha\n",
      "so good day! bad day, I hat\n",
      "so good day! bad day, I hate\n",
      "so good day! bad day, I hate \n",
      "so good day! bad day, I hate i\n",
      "so good day! bad day, I hate it\n",
      "\n",
      "compute log_probs call 0.08078432083129883\n",
      "get top k nodes call: 0.00011491775512695312\n",
      "add children sequences before found branch into working list: 1.0728836059570312e-05\n",
      "log prob nodes: \n",
      "4\n",
      "so good day! bad day, I hate it\n",
      "so good day! bad day, I hate\n",
      "so good day! bad day, I hate \n",
      "so good day! bad day, I hate i\n",
      "\n",
      "get top k nodes call: 5.221366882324219e-05\n",
      "0.42913103103637695\n",
      "\n",
      "\n",
      "['good day! bad day, I hate it', 'good day! bad day, I hate', 'good day! bad day, I hate ', 'good day! bad day, I hate i']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"The weather is amazing! It is so \"\n",
    "context = \"good day! bad day, I hate it\"\n",
    "\n",
    "s = time.time()\n",
    "res = guidance_system.substring(prompt, context, 4)\n",
    "print(time.time() - s)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82b17fba-fd52-4aa9-bac6-44c76acd97a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for select() with choices_list1: 0.09849667549133301 seconds\n",
      "Time taken for select() with choices_list2: 0.061621665954589844 seconds\n",
      "['car', 'ladesh']\n",
      "['car', 'ladesh']\n"
     ]
    }
   ],
   "source": [
    "texts = [\"What usually has 4 wheels? \", \"I have been in country in South Asia called Bang\"]\n",
    "choices_list1 = [[\"car\", \"horse\"], [\"ladesh\", \"cheese\"]]\n",
    "choices_list2 = [[\"horse\", \"car\"], [\"ladesh\", \"cheese\"]]\n",
    "start_time = time.time()\n",
    "output1 = guidance_system.select(texts, choices_list1)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for select() with choices_list1:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "output2 = guidance_system.select(texts, choices_list2)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for select() with choices_list2:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "print(output1)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4fa946f-97d5-482b-b6c1-c9a42437ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for gen(): 0.9958069324493408 seconds\n",
      "['s. I have owned three BMWs in my life, and I would own another one if I']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "gen_res = guidance_system.gen(\"I am a big fan of BMW\", max_length=30, min_length=10)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for gen():\", end_time - start_time, \"seconds\")\n",
    "print(gen_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67246964-5459-45c5-b8c6-73d1ae20a9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = \"How many programming languages does BLOOM support? \"\n",
    "choices_list = [[\"text\", \"46 languages\"]]\n",
    "guidance_system.select(texts, choices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7d215d-f815-4744-8f30-9cb66ab1621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> I am a big fan of BMW's and have owned one for 15 years. I have had the pleasure of driving the new M3 and M5 and they are a blast. I am looking for a used M3 for sale, but I am having a hard time finding one that I like. I am looking for one that is less than 5 years old, and has a manual transmission. I have been to several dealerships, and have looked at several\n",
      "\n",
      "\n",
      "Time taken for gen(): 4.105570316314697 seconds\n",
      "[\"'s and have owned one for 15 years. I have had the pleasure of driving the new M3 and M5 and they are a blast. I am looking for a used M3 for sale, but I am having a hard time finding one that I like. I am looking for one that is less than 5 years old, and has a manual transmission. I have been to several dealerships, and have looked at several\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(guidance_system.tokenizer)\n",
    "\n",
    "start_time = time.time()\n",
    "gen_res = guidance_system.gen(\"I am a big fan of BMW\",\n",
    "                              streamer=streamer,\n",
    "                              stop_keywords=\"BMW\",\n",
    "                              max_length=100)\n",
    "end_time = time.time()\n",
    "print()\n",
    "print()\n",
    "print(\"Time taken for gen():\", end_time - start_time, \"seconds\")\n",
    "print(gen_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f667bad-1b95-40a2-ac4c-d6cccf62f77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
