{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de789ee-53a3-4294-91bd-f7b00dc9f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c26d729-c3ee-4ee8-9a8f-92785b202014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import re\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d590378-da78-4647-a1c3-1109fd98d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    A class representing a node in a tree structure. Each node contains information about its token ID, its parent node,\n",
    "    its children nodes, its depth in the tree, and its cumulative log probability.\n",
    "\n",
    "    Attributes:\n",
    "        token_id (int): The ID of the token associated with this node.\n",
    "        parent_node (Node): The parent node of this node. None if this is the root node.\n",
    "        children (list): A list of child nodes.\n",
    "        depth (int): The depth of this node in the tree.\n",
    "        cum_log_probability (float): The cumulative log probability of this node.\n",
    "        token_sequence (torch.Tensor): A tensor representing the sequence of tokens from the root to this node.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_id: int, parent_node: 'Node', depth: int):\n",
    "        \"\"\"\n",
    "        Initializes a new Node instance.\n",
    "\n",
    "        Args:\n",
    "            token_id (int): The ID of the token associated with this node.\n",
    "            parent_node (Node): The parent node of this node. None if this is the root node.\n",
    "            depth (int): The depth of this node in the tree.\n",
    "        \"\"\"\n",
    "        self.token_id = token_id\n",
    "        self.parent_node = parent_node\n",
    "        self.children = []\n",
    "        self.depth = depth\n",
    "        self.cum_log_probability = None\n",
    "\n",
    "        # Initialize the token_sequence based on the parent node's token_sequence and the current token_id\n",
    "        if depth:\n",
    "            self.token_sequence = torch.cat((parent_node.token_sequence, torch.tensor([self.token_id], dtype=torch.long)))\n",
    "        else:\n",
    "            self.token_sequence = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns a string representation of the node, including its token sequence.\n",
    "\n",
    "        Returns:\n",
    "            str: A string representation of the node.\n",
    "        \"\"\"\n",
    "        return f\"Nodes: {self.token_sequence}, {self.cum_log_probability}\"\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if this node is equal to another node or a tensor.\n",
    "\n",
    "        Args:\n",
    "            other (Node or torch.Tensor): The other node or tensor to compare with.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the nodes are equal, False otherwise.\n",
    "        \"\"\"\n",
    "        if isinstance(other, Node):\n",
    "            return torch.equal(self.token_sequence, other.token_sequence)\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"\"\"\n",
    "        Return the hash based on an immutable attribute. Here, we use the string representation of the token_sequence\n",
    "        because tensors themselves are not hashable and should not be used directly in hash computations if their content\n",
    "        may change.\n",
    "        \"\"\"\n",
    "        return hash(tuple(self.token_sequence.tolist()))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7054b53e-3e47-4037-a42d-4f0e81a979bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "class SubstringEngine:\n",
    "    def __init__(self, llm, tokenizer, mode=False):\n",
    "        self.llm = llm\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = min(len(sorted(list(self.tokenizer.vocab.keys()),\n",
    "                                        key=lambda x: len(x), reverse=True)[0]),\n",
    "                                 10)\n",
    "        self.mode = mode\n",
    "\n",
    "    def _expand_tree(self, parent: Node,\n",
    "                    tokenized_candidates: List[torch.Tensor],\n",
    "                    height: int,\n",
    "                    position: int = 0,\n",
    "                    special_ids: List[int] = []) -> Node:\n",
    "        \"\"\"\n",
    "        Expands the tree from a given parent node by adding child nodes based on the tokenized context.\n",
    "    \n",
    "        Args:\n",
    "            parent (Node): The parent node from which to expand the tree.\n",
    "            tokenized_candidates (List[torch.Tensor]): The tokenized context for the prompt.\n",
    "            height (int): The height of the tree to expand to.\n",
    "            position (int, optional): The current position in the tokenized context. Defaults to 0.\n",
    "            special_ids (List[int], optional): A list of special token IDs to exclude from the tree. Defaults to an empty list.\n",
    "    \n",
    "        Returns:\n",
    "            Node: The parent node with its children expanded.\n",
    "        \"\"\"\n",
    "        # Iterate over each context in the tokenized context\n",
    "        for candidate in tokenized_candidates:\n",
    "            # Get the token at the current position\n",
    "            if position < len(candidate):\n",
    "                token = candidate[position].item()\n",
    "                # Check if the token is not a special token and if it's not already a child of the parent\n",
    "                if (torch.equal(candidate[:position], parent.token_sequence) and \n",
    "                    all(token != child.token_id for child in parent.children) and\n",
    "                    token not in special_ids):\n",
    "                    \n",
    "                    # Create a new node with the current token and add it as a child to the parent\n",
    "                    new_node = Node(token, parent, parent.depth + 1)\n",
    "                    parent.children.append(new_node)\n",
    "        \n",
    "                    # Recursively expand the tree if the current position is less than the height\n",
    "                    if new_node.depth < height:\n",
    "                        self._expand_tree(new_node, tokenized_candidates, height, position + 1, special_ids)\n",
    "        # Return the parent node with its children expanded\n",
    "        return parent\n",
    "\n",
    "    \n",
    "    def _build_tree(self, tokenized_context: List[torch.Tensor]) -> Tuple[Node, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Builds the entire tree for a given prompt using the tokenized context.\n",
    "    \n",
    "        Args:\n",
    "            promt (str): The prompt for which the tree is being built.\n",
    "            tokenized_context (List[torch.Tensor]): The tokenized context for the prompt.    \n",
    "        Returns:\n",
    "            Tuple[Node, torch.Tensor]: The root node of the tree and the tokenized prompt.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = time.time()\n",
    "        \n",
    "        # Initialize the root node and tokenize the prompt\n",
    "        root = Node(-1, None, 0)\n",
    "        # Expand the tree from the root node to the specified height, excluding special tokens\n",
    "        root = self._expand_tree(root,\n",
    "                           tokenized_context,\n",
    "                           len(tokenized_context[0]),\n",
    "                           special_ids = self.tokenizer.all_special_ids)\n",
    "        # Set the cumulative log probability of the root node to 0\n",
    "        root.cum_log_probability = 0\n",
    "        \n",
    "        if self.mode:\n",
    "            print(f\"build tree for first tokens: {time.time() - s}\")\n",
    "        \n",
    "        # Return the root node and the tokenized prompt\n",
    "        return root\n",
    "\n",
    "\n",
    "    def _candidate_sequences(self, context, max_token_length, prompt=''):\n",
    "        \"\"\"\n",
    "        Generates a set of candidate sequences from the given context by considering all\n",
    "        possible substrings within a specified length limit.\n",
    "        \n",
    "        These candidates are then prefixed with the provided prompt to form complete sequences.\n",
    "    \n",
    "        Args:\n",
    "            context (str): The input context from which to generate candidate sequences.\n",
    "            max_token_length (int): The maximum length of a candidate sequence in terms of tokens.\n",
    "            prompt (str, optional): A prefix to be added to each candidate sequence. Defaults to an empty string.\n",
    "    \n",
    "        Returns:\n",
    "            list: A list of candidate sequences, each starting with the provided prompt.\n",
    "    \n",
    "        The function first calculates the restriction based on the length of the context and the maximum token length.\n",
    "        It then iterates over the text to generate all possible substrings within this restriction.\n",
    "        These substrings are added to a set to ensure uniqueness. The set is then sorted for reproducibility,\n",
    "        and each candidate is prefixed with the prompt to form complete\n",
    "        sequences. These sequences are returned as a list.\n",
    "        \"\"\"\n",
    "        s = time.time()\n",
    "        # Calculate the restriction based on the length of the text and the maximum token length\n",
    "        restriction = min(len(context) + 1, max_token_length)\n",
    "        # Initialize an empty set to store unique substring candidates\n",
    "        substring_candidates = set()\n",
    "        # Iterate over the text to generate all possible substrings within the restriction\n",
    "        for i in range(len(context)):\n",
    "            for j in range(i+1, i+restriction):\n",
    "                substring_candidates.add(context[i:j])\n",
    "        \n",
    "        # Sort the set of substring candidates for reproducibility\n",
    "        substring_candidates = sorted(substring_candidates)\n",
    "        # Prefix each candidate with the prompt to form complete sequences\n",
    "        sequences = [prompt + candidate for candidate in substring_candidates]\n",
    "\n",
    "        if self.mode:\n",
    "            print(f\"get candidates: last 2 tokens + all substring candidates {time.time() - s}\")\n",
    "            \n",
    "        return sequences\n",
    "\n",
    "    \n",
    "    def _compute_logprob(self, common_part, nodes):\n",
    "        \"\"\"\n",
    "        Computes the cumulative log probabilities for each node in the tree structure,\n",
    "        given a common part of the text (user prompt wihtout last 2 tokens) and a list of nodes.\n",
    "    \n",
    "        This function is crucial for evaluating the likelihood of each candidate sequence generated from the context text.\n",
    "        It does so by leveraging the transformer model to predict the next token in the sequence and then calculating\n",
    "        the log probability of each token. \n",
    "        \n",
    "        Args:\n",
    "            common_part (str): A common part of the text (user prompt wihtout last 2 tokens) that is shared by all\n",
    "                               nodes in the tree. This is used to ensure that the model's predictions are relevant\n",
    "                               to the context of the input prompt.\n",
    "            nodes (List[Node]): A list of nodes for which the cumulative log probabilities are to be computed.\n",
    "    \n",
    "        Returns:\n",
    "            None: The function modifies the nodes in-place, updating their cumulative log probabilities.\n",
    "    \n",
    "        The function begins by initializing an empty list for the input batch and two empty lists for mapping nodes\n",
    "        to their corresponding log probabilities and input sequences. It then iterates over each node, checking if\n",
    "        its cumulative log probability has been set. If not, it constructs the input sequence for the model by \n",
    "        concatenating the common part of the text with the token sequence of the node. This input sequence is then\n",
    "        added to the input batch and the node is mapped to its corresponding log probability.\n",
    "    \n",
    "        Once all nodes have been processed, the function tokenizes the input batch using the tokenizer and feeds\n",
    "        it into the model to get the logits. The log probabilities are then calculated using the log_softmax function.\n",
    "    \n",
    "        Finally, the function iterates over the nodes again, this time updating their cumulative log probabilities\n",
    "        based on the log probabilities of their tokens and the cumulative log probabilities of their parent nodes.\n",
    "        \n",
    "        This process ensures that each node's cumulative log probability reflects the likelihood\n",
    "        of the sequence of tokens leading up to it.\n",
    "        \"\"\"\n",
    "        def process_log_probs(token_ids, logits):\n",
    "            for token_id in nodes_to_gather:\n",
    "                log_probs_mapping[token_id] = logits[token_id.token_id]\n",
    "            return logits\n",
    "   \n",
    "        if self.mode:\n",
    "            print(\"log prob nodes: \")\n",
    "            print(len(nodes))\n",
    "            for node in nodes:\n",
    "                print(self.tokenizer.decode(node.token_sequence))\n",
    "            print()\n",
    "\n",
    "        s = time.time()\n",
    "        \n",
    "        # Calculate the number of tokens in the common part of the text\n",
    "        skip_logits = len(self.tokenizer.encode(common_part))\n",
    "        log_probs_mapping = {}\n",
    "        inputs_map = {}\n",
    "        common_part_encoded = self.tokenizer.encode(common_part)\n",
    "    \n",
    "        # Iterate over each node\n",
    "        for node in nodes:\n",
    "            # Check if the node's cumulative log probability has been set\n",
    "            if node.cum_log_probability is None:\n",
    "                # Construct the input sequence for the model\n",
    "                # we get tokens of the parent, since we need only\n",
    "                # them for getting log probability for current considered node token\n",
    "                inp = common_part_encoded + node.parent_node.token_sequence.tolist()\n",
    "                inputs_map.setdefault(tuple(inp), []).append(node)\n",
    "    \n",
    "        # If there are no nodes to process, return\n",
    "        if not inputs_map:\n",
    "            return\n",
    "    \n",
    "        sampling_params_for_leafs = SamplingParams(max_tokens=1,\n",
    "                                         prompt_logprobs=1,\n",
    "                                         logits_processors=[process_log_probs])\n",
    "        \n",
    "        sampling_params_for_empty_parents = SamplingParams(max_tokens=1,\n",
    "                                                           prompt_logprobs=1)\n",
    "\n",
    "        # Feed the input into the model to get the logits\n",
    "        with torch.no_grad():            \n",
    "            for inp in inputs_map:\n",
    "                nodes_to_gather = inputs_map[inp]\n",
    "                self.llm.generate(prompts=None,\n",
    "                                  prompt_token_ids=[list(inp)],\n",
    "                                  sampling_params=sampling_params_for_leafs)\n",
    "            \n",
    "        # Iterate over the nodes again to update their cumulative log probabilities\n",
    "        for idx, inp in enumerate(inputs_map):\n",
    "            for node in inputs_map[inp]:\n",
    "                # if self.mode:\n",
    "                #     print(\"seq:\")\n",
    "                #     print(node.token_sequence)\n",
    "                #     print()\n",
    "                    \n",
    "                # It is possible, that parent node cum_log_probability is not calculated yet\n",
    "                # Thus, if it is a such situation, we will calculate log_probability for parent nodes also\n",
    "                \n",
    "                # Initialize lists for the parent nodes and their log probabilities\n",
    "                parents_log_probs = []\n",
    "                parents_sequence_without_logprob = []\n",
    "                # Iterate over the parent nodes\n",
    "                parent_tmp = node.parent_node\n",
    "                while parent_tmp.cum_log_probability is None:\n",
    "                    parents_sequence_without_logprob.append(parent_tmp)\n",
    "                    parent_tmp = parent_tmp.parent_node\n",
    "\n",
    "\n",
    "                if parents_sequence_without_logprob:\n",
    "                    prompt_input_ids = common_part_encoded + parents_sequence_without_logprob[0].token_sequence.tolist()\n",
    "                    outputs = self.llm.generate(prompts=None,\n",
    "                                                prompt_token_ids=[prompt_input_ids],\n",
    "                                                sampling_params=sampling_params_for_empty_parents)[0]\n",
    "                \n",
    "                # Calculate the cumulative log probability for each parent node\n",
    "                number_of_parents_without_logbrob = len(parents_sequence_without_logprob)\n",
    "                for n_id in range(number_of_parents_without_logbrob - 1, -1, -1):\n",
    "                    t_id = parents_sequence_without_logprob[n_id].token_id\n",
    "                    # print(outputs.prompt_logprobs[-1 - n_id])\n",
    "                    parents_sequence_without_logprob[n_id].cum_log_probability = (outputs.prompt_logprobs[-1 - n_id][t_id].logprob + \n",
    "                                                                                  parents_sequence_without_logprob[n_id].parent_node.cum_log_probability)\n",
    "                   \n",
    "                # Update the node's cumulative log probability                \n",
    "                # if self.mode:\n",
    "                #     print(\"children: \")\n",
    "                #     print(node.token_id)\n",
    "                    \n",
    "                node.cum_log_probability = log_probs_mapping[node] + node.parent_node.cum_log_probability\n",
    "\n",
    "        if self.mode:\n",
    "            print(f\"compute log_probs call {time.time() - s}\")\n",
    "            \n",
    "    \n",
    "    def _get_topk_nodes(self, nodes, k):\n",
    "        \"\"\"\n",
    "        Selects the top `k` nodes from a given list of nodes based on their cumulative log probabilities, normalized by their depth.\n",
    "    \n",
    "        This function is used to prune the tree and focus on the most promising candidates for further evaluation or output.\n",
    "        By normalizing the cumulative log probabilities by the depth of each node,\n",
    "        it ensures that nodes deeper in the tree are not overly favored simply because they are longer.\n",
    "    \n",
    "        Args:\n",
    "            nodes (List[Node]): A list of nodes from which to select the top `k` nodes.\n",
    "            k (int): The number of top nodes to select.\n",
    "    \n",
    "        Returns:\n",
    "            List[Node]: A list of the top `k` nodes, sorted by their normalized cumulative log probabilities.\n",
    "    \n",
    "        The function begins by calculating the normalized scores for each node.\n",
    "        This is done by dividing the cumulative log probability of each node by its depth.\n",
    "        The scores are then converted into a tensor and the indices of the top `k` scores \n",
    "        are determined using the `torch.topk` function. These indices are used to select \n",
    "        the corresponding nodes from the original list.\n",
    "    \n",
    "        The selected nodes are returned as a list, which can then be used for further processing\n",
    "        or output. This function is particularly useful in scenarios where the tree is large and \n",
    "        contains many nodes, allowing the script to efficiently focus on the most likely candidates.\n",
    "        \"\"\"\n",
    "        s = time.time()\n",
    "        # Calculate the normalized scores for each node\n",
    "        scores = torch.tensor([node.cum_log_probability/node.depth for node in nodes])\n",
    "        # Determine the indices of the top k scores\n",
    "        top_k_indices = torch.topk(scores, k=k, largest=True, sorted=True).indices\n",
    "        \n",
    "        if self.mode:\n",
    "            print(f\"get top k nodes call: {time.time() - s}\")\n",
    "        # Select the top k nodes using the indices\n",
    "        return [nodes[i] for i in top_k_indices]\n",
    "\n",
    "\n",
    "    def _candidate_sequences_exp(self, context, chosen_options, max_candidate_length, prompt=''):\n",
    "        \"\"\"\n",
    "        Generates a set of candidate sequences from the given context,\n",
    "        with each candidate starting with one of the chosen options.\n",
    "    \n",
    "        This function is useful for scenarios where the context or the prompt suggests\n",
    "        specific starting points for the sequences, allowing for more targeted generation.\n",
    "        \n",
    "        It iterates over the text to generate all possible substrings within a specified length\n",
    "        limit and checks if each candidate starts with one of the chosen options. Only those \n",
    "        candidates that meet this condition are added to the set of substring candidates.\n",
    "    \n",
    "        Args:\n",
    "            context (str): The input context from which to generate candidate sequences.\n",
    "            chosen_options (List[str]): A list of options that each candidate sequence must start with.\n",
    "            max_candidate_length (int): The maximum length of a candidate sequence in terms of tokens.\n",
    "            prompt (str, optional): A prefix to be added to each candidate sequence. Defaults to an empty string.\n",
    "    \n",
    "        Returns:\n",
    "            list: A list of candidate sequences, each starting with one of the chosen options and prefixed with the provided prompt.\n",
    "    \n",
    "        The function first calculates the restriction based on the length of the text and the maximum candidate length. It then iterates over the text to generate all possible substrings within this restriction. For each substring, it checks if the substring starts with one of the chosen options. If so, the substring is added to the set of substring candidates. The set is then sorted for reproducibility, and each candidate is prefixed with the prompt to form complete sequences. These sequences are returned as a list.\n",
    "        \"\"\"\n",
    "        s = time.time()\n",
    "        # Calculate the restriction based on the length of the text and the maximum candidate length\n",
    "        restriction = min(len(context) + 1, max_candidate_length)\n",
    "        # Initialize an empty set to store unique substring candidates\n",
    "        substring_candidates = set()\n",
    "        # Iterate over the text to generate all possible substrings within the restriction\n",
    "        for i in range(len(context)):\n",
    "            for j in range(i+1, i+restriction):\n",
    "                candidate = prompt + context[i:j]\n",
    "                tokenized_candidate = self.tokenizer.encode(candidate, return_tensors=\"pt\", add_special_tokens=False)[0]\n",
    "                # Check if the candidate starts with one of the chosen options\n",
    "                if any(torch.equal(tokenized_candidate[:len(option)], option) for option in chosen_options):\n",
    "                    substring_candidates.add(candidate)\n",
    "        \n",
    "        # Sort the set of substring candidates for reproducibility\n",
    "        sequences = sorted(list(substring_candidates))\n",
    "\n",
    "        if self.mode:\n",
    "            print(f\"get expanded candidates starts with top k first tokens: {time.time() - s}\")\n",
    "        return sequences\n",
    "\n",
    "    \n",
    "    def _get_nodes_seq_before_branch(self, node):\n",
    "        \"\"\"\n",
    "        Traverses the tree structure from a given node and returns the node that is just before a branching point.\n",
    "    \n",
    "        Args:\n",
    "            node (Node): The starting node from which to traverse the tree.\n",
    "    \n",
    "        Returns:\n",
    "            Node: The node that is just before a branching point in the tree.\n",
    "    \n",
    "        The function begins by entering a loop that continues until it finds a node with more than one child.\n",
    "        It starts with the given node and checks its children. If the node has exactly one child, the function \n",
    "        moves to that child and continues the process. This ensures that the function traverses down the tree \n",
    "        until it reaches a node that is about to branch into multiple paths.\n",
    "    \n",
    "        Once the branching point is found, the function breaks out of the loop and returns the node that led to \n",
    "        this branching. This node is the one just before the branching point, and it can be used for further \n",
    "        processing or analysis.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            children = node.children\n",
    "            # If the node has exactly one child, move to that child\n",
    "            if len(children) == 1:\n",
    "                node = children[0]\n",
    "            else:\n",
    "                # If the node has more than one child, it's a branching point\n",
    "                break\n",
    "        # Return the node just before the branching point\n",
    "        return node\n",
    "\n",
    "\n",
    "    def _iteration(self, working_list, common_part, k):\n",
    "        \"\"\"\n",
    "        Performs an iteration of the sequence generation process by computing the cumulative log probabilities\n",
    "        of the nodes in the working list and selecting the top `k` nodes.\n",
    "    \n",
    "        Args:\n",
    "            working_list (List[Node]): The list of nodes for which the cumulative log probabilities are to be computed\n",
    "                                       and from which the top `k` nodes are to be selected.\n",
    "            common_part (str): A common part of the text that is shared by all nodes in the tree. This is used to ensure\n",
    "                               that the model's predictions are relevant to the context of the input text.\n",
    "            k (int): The number of top nodes to select.\n",
    "    \n",
    "        Returns:\n",
    "            List[Node]: The top `k` nodes from the working list, sorted by their normalized cumulative log probabilities.\n",
    "        \"\"\"\n",
    "        # Add children to wotking list for every node in it\n",
    "        working_list = self._update_working_list_with_children(working_list)\n",
    "        \n",
    "        # Compute the cumulative log probabilities for each node in the working list\n",
    "        self._compute_logprob(common_part, working_list)\n",
    "        # Select the top k nodes from the working list based on their cumulative log probabilities\n",
    "        return self._get_topk_nodes(working_list, k)\n",
    "\n",
    "\n",
    "    def _update_working_list_with_children(self, working_list):\n",
    "        \"\"\"\n",
    "        Updates the working list of nodes by adding the children of each node in the list,\n",
    "        specifically those that are just before a branching point in the tree.\n",
    "    \n",
    "        Args:\n",
    "            working_list (List[Node]): The current working list of nodes to be updated.\n",
    "    \n",
    "        Returns:\n",
    "            List[Node]: The updated working list of nodes, including the children of each node in the original list.\n",
    "        \"\"\"\n",
    "        s = time.time()\n",
    "        for node in working_list:\n",
    "            for c in node.children:\n",
    "                candidate_to_add = self._get_nodes_seq_before_branch(c)\n",
    "                if candidate_to_add not in working_list:\n",
    "                    working_list.append(candidate_to_add)\n",
    "\n",
    "        if self.mode:\n",
    "            print(f\"add children sequences before found branch into working list: {time.time() - s}\")\n",
    "        return working_list\n",
    "\n",
    "    \n",
    "    def substring(self, prompt, context, k, max_substring_length, return_full_text=False):\n",
    "        \"\"\"\n",
    "        Generates and evaluates candidate sequences based on a given prompt and context, selecting the top `k` nodes.\n",
    "    \n",
    "        Args:\n",
    "            prompt (str): The prompt for which the tree is being built and from which the last part and the common part are extracted.\n",
    "            context (str): The context from which candidate sequences are generated.\n",
    "            k (int): The number of top nodes to select.\n",
    "            max_substring_length (int): The maximum length of a candidate sequence in terms of tokens.\n",
    "    \n",
    "        Returns:\n",
    "            Tuple[List[Node], Node]: The top `k` nodes from the working list and the initial root node of the tree.\n",
    "        \"\"\"\n",
    "        # Tokenize the prompt and extract the last part and the common part\n",
    "        tokenized_prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, add_special_tokens=False)['input_ids']\n",
    "        last_part = self.tokenizer.decode(tokenized_prompt[0, -2:])\n",
    "        common_part = self.tokenizer.decode(tokenized_prompt[0, :-2])\n",
    "\n",
    "        if len(context) == 1:\n",
    "            if return_full_text:\n",
    "                return prompt + context\n",
    "            return context\n",
    "        \n",
    "        # Generate candidate sequences from the context\n",
    "        substring_candidates = self._candidate_sequences(context, self.max_token_len, last_part)\n",
    "        tokenized_s_cand = self.tokenizer(substring_candidates, return_tensors=\"pt\", padding=True, add_special_tokens=False)['input_ids']\n",
    "\n",
    "        print(substring_candidates)\n",
    "    \n",
    "        # Build the tree structure from the tokenized candidate sequences\n",
    "        initial_root = self._build_tree(tokenized_s_cand)\n",
    "\n",
    "        # Expand the tree from the root node\n",
    "        node_before_branch = self._get_nodes_seq_before_branch(initial_root)\n",
    "        first_branch_children = node_before_branch.children\n",
    "        \n",
    "        if not first_branch_children:\n",
    "            first_branch_children = [node_before_branch]\n",
    "            \n",
    "        # Last token of the prompt can be changed\n",
    "        # Therefore, we have to capture not tokens before first branch, but all its children after branch    \n",
    "        \n",
    "        working_list = []\n",
    "        for node in first_branch_children:\n",
    "            wl_len = len(working_list)\n",
    "            \n",
    "            for c in node.children:\n",
    "                candidate_to_add = self._get_nodes_seq_before_branch(c)\n",
    "                if candidate_to_add not in working_list:\n",
    "                    working_list.append(candidate_to_add)  \n",
    "                    \n",
    "            if wl_len == len(working_list):\n",
    "                working_list.append(node)\n",
    "                   \n",
    "        self._compute_logprob(common_part, working_list)\n",
    "        working_list = self._get_topk_nodes(working_list, k)\n",
    "    \n",
    "        # Generate expanded candidate sequences based on the chosen candidates\n",
    "        chosen_candidates = list(map(lambda x: x.token_sequence, working_list))\n",
    "\n",
    "        # if self.mode:\n",
    "        #     print(\"chosen_candidates for explansion: \")\n",
    "        #     for c in chosen_candidates:\n",
    "        #         print(self.tokenizer.decode(c))\n",
    "        #     print()\n",
    "        \n",
    "        expanded_candidates = self._candidate_sequences_exp(context, chosen_candidates, max_substring_length, last_part)\n",
    "        \n",
    "        tokenized_expanded_candidates = self.tokenizer(expanded_candidates, return_tensors=\"pt\", padding=True, add_special_tokens=False)['input_ids']\n",
    "    \n",
    "        # Update the working list with children nodes\n",
    "        working_list[0].parent_node.children = working_list\n",
    "    \n",
    "        # Expand the tree with the expanded candidate sequences\n",
    "        for node in working_list:\n",
    "            self._expand_tree(node,\n",
    "                        tokenized_expanded_candidates,\n",
    "                        len(tokenized_expanded_candidates[0]),\n",
    "                        position = node.depth,\n",
    "                        special_ids=self.tokenizer.all_special_ids)\n",
    "    \n",
    "        # Iteratively refine the set of candidate sequences based on their likelihood\n",
    "        while True:\n",
    "            prev_w_l = deepcopy(working_list)\n",
    "            working_list = self._iteration(working_list, common_part, k)\n",
    "            if prev_w_l == working_list:\n",
    "                break\n",
    "    \n",
    "        res_text = []\n",
    "        for node in working_list:\n",
    "            substirng_choice = self.tokenizer.decode(node.token_sequence)\n",
    "            \n",
    "            if return_full_text:\n",
    "                res_text.append(common_part + substirng_choice)\n",
    "            else:\n",
    "                res_text.append(substirng_choice[len(last_part):])\n",
    "        \n",
    "        return res_text, working_list, initial_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da1ed9f-a16d-4918-8b23-3118d069a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria\n",
    "\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, eos_sequence, len_tokenized_eos_sequence, tokenizer):\n",
    "        self.eos_sequence = eos_sequence\n",
    "        self.len_tokenized_eos_sequence = len_tokenized_eos_sequence\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self,\n",
    "                 input_ids: torch.LongTensor,\n",
    "                 scores: torch.FloatTensor,\n",
    "                 **kwargs) -> bool:\n",
    "        \n",
    "        # Check each batch item if the sequence ends with the specified eos_sequence\n",
    "        last_ids = self.tokenizer.decode(input_ids[:,-self.len_tokenized_eos_sequence:])\n",
    "        # Check if all elements in eos_sequence match for any item in the batch\n",
    "        return self.eos_sequence == last_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f2f130-2976-4f4d-8a15-25d94c3170d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidanceBeta:\n",
    "    \"\"\"\n",
    "    Class for generating guidance using a pretrained language model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Pretrained model identifier from Hugging Face model hub.\n",
    "        mode (bool): Mode for the guidance generation (whether to print log messages or not).\n",
    "        model_kwargs (dict): Additional keyword arguments to pass to the model initialization.\n",
    "        tokenizer_kwargs (dict): Additional keyword arguments to pass to the tokenizer initialization.\n",
    "\n",
    "    Attributes:\n",
    "        model (AutoModelForCausalLM): Pretrained model for generating guidance.\n",
    "        tokenizer (AutoTokenizer): Tokenizer for tokenizing inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 llm,\n",
    "                 mode=True):\n",
    "        \n",
    "        self.llm = llm\n",
    "        \n",
    "        if hasattr(llm, \"get_tokenizer\"):\n",
    "            self.tokenizer = self.llm.get_tokenizer()\n",
    "        elif hasattr(llm, \"tokenizer\"):\n",
    "            if hasattr(llm.tokenizer, \"tokenizer\"):\n",
    "                self.tokenizer = self.llm.tokenizer.tokenizer\n",
    "            else:\n",
    "                self.tokenizer = self.llm.tokenizer\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The provided LLM instance in RegexLogitsProcessor neither has a \"\n",
    "                \"`tokenizer` attribute or a get_tokenizer method.\"\n",
    "            )\n",
    "\n",
    "        if not self.tokenizer.pad_token:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.mode = mode\n",
    "\n",
    "    def substring(self, input_text, context:str, k=1, max_substring_length=35):\n",
    "        substring_engine = SubstringEngine(self.llm, self.tokenizer, mode=self.mode)\n",
    "        # return res_text, working_list, initial_root\n",
    "        result = substring_engine.substring(input_text, context, k, max_substring_length)\n",
    "        \n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25ed5651-9aa4-48a4-8f40-1475e05f78e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_TOKEN=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\n",
      "INFO 05-21 15:11:53 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='meta-llama/Meta-Llama-3-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 15:11:53 utils.py:608] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-21 15:11:54 selector.py:28] Using FlashAttention backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W CUDAAllocatorConfig.h:30] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-21 15:11:55 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 05-21 15:11:57 model_runner.py:173] Loading model weights took 14.9595 GB\n",
      "INFO 05-21 15:11:58 gpu_executor.py:119] # GPU blocks: 3757, # CPU blocks: 2048\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from vllm import LLM\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "access_token = \"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\"\n",
    "\n",
    "%env HF_TOKEN=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\n",
    "\n",
    "# Specify the primary GPU to use (GPU 0, which has more available memory)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Setting PyTorch environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=1,  # Consider adjusting if still facing issues\n",
    "    gpu_memory_utilization=0.3,\n",
    "    enforce_eager=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a5ec3d-bcc5-4f8a-a21d-201b88133b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_system = GuidanceBeta(llm, mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c251854-47ec-4f73-aee1-1f9567891f79",
   "metadata": {},
   "source": [
    "Сейчас на сабстринге включены принты, чтобы следить за временем каждого этапа алогоритма.\n",
    "Если хочешь выключить, то передай в инициализацию GuidanceBeta аргумент `mode=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0120e96b-f9ef-419f-9215-b493282a5ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get candidates: last 2 tokens + all substring candidates 0.0001735687255859375\n",
      "[': \" ', ': \" в', ': \" в ', ': \" в п', ': \" в пр', ': \" в про', ': \" в проц', ': \" в проце', ': \" в процес', ': \" з', ': \" за', ': \" зад', ': \" зада', ': \" задач', ': \" задачу', ': \" задачу ', ': \" задачу к', ': \" к', ': \" ко', ': \" кон', ': \" конф', ': \" конфе', ': \" конфет', ': \" конфет,', ': \" конфет, ', ': \" ку', ': \" куп', ': \" купи', ': \" купит', ': \" купить', ': \" купить ', ': \" купить к', ': \" п', ': \" пр', ': \" про', ': \" проц', ': \" проце', ': \" процес', ': \" процесс', ': \" процессе', ': \" с', ': \" ст', ': \" ста', ': \" стат', ': \" стату', ': \" статус', ': \" статус ', ': \" статус в', ': \",', ': \", ', ': \", с', ': \", ст', ': \", ста', ': \", стат', ': \", стату', ': \", статус', ': \", статус ', ': \"С', ': \"Со', ': \"Соз', ': \"Созд', ': \"Созда', ': \"Создай', ': \"Создай ', ': \"Создай з', ': \"Создай за', ': \"а', ': \"ад', ': \"ада', ': \"адач', ': \"адачу', ': \"адачу ', ': \"адачу к', ': \"адачу ку', ': \"адачу куп', ': \"ай', ': \"ай ', ': \"ай з', ': \"ай за', ': \"ай зад', ': \"ай зада', ': \"ай задач', ': \"ай задачу', ': \"ат', ': \"ату', ': \"атус', ': \"атус ', ': \"атус в', ': \"атус в ', ': \"атус в п', ': \"атус в пр', ': \"ач', ': \"ачу', ': \"ачу ', ': \"ачу к', ': \"ачу ку', ': \"ачу куп', ': \"ачу купи', ': \"ачу купит', ': \"в', ': \"в ', ': \"в п', ': \"в пр', ': \"в про', ': \"в проц', ': \"в проце', ': \"в процес', ': \"в процесс', ': \"д', ': \"да', ': \"дай', ': \"дай ', ': \"дай з', ': \"дай за', ': \"дай зад', ': \"дай зада', ': \"дай задач', ': \"дач', ': \"дачу', ': \"дачу ', ': \"дачу к', ': \"дачу ку', ': \"дачу куп', ': \"дачу купи', ': \"е', ': \"ес', ': \"есс', ': \"ессе', ': \"ет', ': \"ет,', ': \"ет, ', ': \"ет, с', ': \"ет, ст', ': \"ет, ста', ': \"ет, стат', ': \"ет, стату', ': \"з', ': \"за', ': \"зад', ': \"зада', ': \"задач', ': \"задачу', ': \"задачу ', ': \"задачу к', ': \"задачу ку', ': \"зд', ': \"зда', ': \"здай', ': \"здай ', ': \"здай з', ': \"здай за', ': \"здай зад', ': \"здай зада', ': \"и', ': \"ит', ': \"ить', ': \"ить ', ': \"ить к', ': \"ить ко', ': \"ить кон', ': \"ить конф', ': \"ить конфе', ': \"й', ': \"й ', ': \"й з', ': \"й за', ': \"й зад', ': \"й зада', ': \"й задач', ': \"й задачу', ': \"й задачу ', ': \"к', ': \"ко', ': \"кон', ': \"конф', ': \"конфе', ': \"конфет', ': \"конфет,', ': \"конфет, ', ': \"конфет, с', ': \"ку', ': \"куп', ': \"купи', ': \"купит', ': \"купить', ': \"купить ', ': \"купить к', ': \"купить ко', ': \"н', ': \"нф', ': \"нфе', ': \"нфет', ': \"нфет,', ': \"нфет, ', ': \"нфет, с', ': \"нфет, ст', ': \"нфет, ста', ': \"о', ': \"оз', ': \"озд', ': \"озда', ': \"оздай', ': \"оздай ', ': \"оздай з', ': \"оздай за', ': \"оздай зад', ': \"он', ': \"онф', ': \"онфе', ': \"онфет', ': \"онфет,', ': \"онфет, ', ': \"онфет, с', ': \"онфет, ст', ': \"оц', ': \"оце', ': \"оцес', ': \"оцесс', ': \"оцессе', ': \"п', ': \"пи', ': \"пит', ': \"пить', ': \"пить ', ': \"пить к', ': \"пить ко', ': \"пить кон', ': \"пить конф', ': \"пр', ': \"про', ': \"проц', ': \"проце', ': \"процес', ': \"процесс', ': \"процессе', ': \"р', ': \"ро', ': \"роц', ': \"роце', ': \"роцес', ': \"роцесс', ': \"роцессе', ': \"с', ': \"с ', ': \"с в', ': \"с в ', ': \"с в п', ': \"с в пр', ': \"с в про', ': \"с в проц', ': \"с в проце', ': \"се', ': \"сс', ': \"ссе', ': \"ст', ': \"ста', ': \"стат', ': \"стату', ': \"статус', ': \"статус ', ': \"статус в', ': \"статус в ', ': \"т', ': \"т,', ': \"т, ', ': \"т, с', ': \"т, ст', ': \"т, ста', ': \"т, стат', ': \"т, стату', ': \"т, статус', ': \"та', ': \"тат', ': \"тату', ': \"татус', ': \"татус ', ': \"татус в', ': \"татус в ', ': \"татус в п', ': \"ту', ': \"тус', ': \"тус ', ': \"тус в', ': \"тус в ', ': \"тус в п', ': \"тус в пр', ': \"тус в про', ': \"ть', ': \"ть ', ': \"ть к', ': \"ть ко', ': \"ть кон', ': \"ть конф', ': \"ть конфе', ': \"ть конфет', ': \"у', ': \"у ', ': \"у к', ': \"у ку', ': \"у куп', ': \"у купи', ': \"у купит', ': \"у купить', ': \"у купить ', ': \"уп', ': \"упи', ': \"упит', ': \"упить', ': \"упить ', ': \"упить к', ': \"упить ко', ': \"упить кон', ': \"ус', ': \"ус ', ': \"ус в', ': \"ус в ', ': \"ус в п', ': \"ус в пр', ': \"ус в про', ': \"ус в проц', ': \"ф', ': \"фе', ': \"фет', ': \"фет,', ': \"фет, ', ': \"фет, с', ': \"фет, ст', ': \"фет, ста', ': \"фет, стат', ': \"ц', ': \"це', ': \"цес', ': \"цесс', ': \"цессе', ': \"ч', ': \"чу', ': \"чу ', ': \"чу к', ': \"чу ку', ': \"чу куп', ': \"чу купи', ': \"чу купит', ': \"чу купить', ': \"ь', ': \"ь ', ': \"ь к', ': \"ь ко', ': \"ь кон', ': \"ь конф', ': \"ь конфе', ': \"ь конфет', ': \"ь конфет,']\n",
      "build tree for first tokens: 0.6633615493774414\n",
      "log prob nodes: \n",
      "85\n",
      ": \" \n",
      ": \" в\n",
      ": \" з\n",
      ": \" за\n",
      ": \" задачу\n",
      ": \" зада\n",
      ": \" к\n",
      ": \" ко\n",
      ": \" кон\n",
      ": \" конф\n",
      ": \" ку\n",
      ": \" куп\n",
      ": \" п\n",
      ": \" пр\n",
      ": \" про\n",
      ": \" проце\n",
      ": \" процес\n",
      ": \" процесс\n",
      ": \" процессе\n",
      ": \" с\n",
      ": \" стату\n",
      ": \" ста\n",
      ": \" стат\n",
      ": \" статус\n",
      ": \"С\n",
      ": \"а\n",
      ": \"адачу\n",
      ": \"ада\n",
      ": \"ай\n",
      ": \"атус\n",
      ": \"ату\n",
      ": \"ачу\n",
      ": \"в\n",
      ": \"дай\n",
      ": \"да\n",
      ": \"е\n",
      ": \"ес\n",
      ": \"ессе\n",
      ": \"ет,\n",
      ": \"з\n",
      ": \"за\n",
      ": \"здай\n",
      ": \"и\n",
      ": \"ит\n",
      ": \"ить\n",
      ": \"й\n",
      ": \"к\n",
      ": \"ко\n",
      ": \"конф\n",
      ": \"купи\n",
      ": \"куп\n",
      ": \"нф\n",
      ": \"о\n",
      ": \"оз\n",
      ": \"озд\n",
      ": \"онф\n",
      ": \"п\n",
      ": \"пи\n",
      ": \"пр\n",
      ": \"р\n",
      ": \"ро\n",
      ": \"с\n",
      ": \"се\n",
      ": \"ст\n",
      ": \"ста\n",
      ": \"т\n",
      ": \"та\n",
      ": \"ту\n",
      ": \"ть\n",
      ": \"у\n",
      ": \"уп\n",
      ": \"ус\n",
      ": \"ф\n",
      ": \"цессе\n",
      ": \"це\n",
      ": \"цес\n",
      ": \"ч\n",
      ": \"чу\n",
      ": \"ь\n",
      ": \", \n",
      ": \", с\n",
      ": \", стату\n",
      ": \", ста\n",
      ": \", стат\n",
      ": \", статус \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 32.22it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.24it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.13it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.33it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.51it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.44it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.34it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.93it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.33it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.91it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 36.95it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.36it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.25it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.51it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 36.68it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.33it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.57it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.84it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.01it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.14it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.23it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 36.38it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.18it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.17it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.95it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.31it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.73it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.05it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.49it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.27it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute log_probs call 1.1458938121795654\n",
      "get top k nodes call: 0.001451730728149414\n",
      "get expanded candidates starts with top k first tokens: 0.12157893180847168\n",
      "add children sequences before found branch into working list: 3.552436828613281e-05\n",
      "log prob nodes: \n",
      "9\n",
      ": \"в\n",
      ": \"в \n",
      ": \"в п\n",
      ": \"в пр\n",
      ": \"в про\n",
      ": \"в проце\n",
      ": \"в процес\n",
      ": \"в процесс\n",
      ": \"в процессе\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.72it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.69it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 37.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute log_probs call 0.08724117279052734\n",
      "get top k nodes call: 0.0007109642028808594\n",
      "add children sequences before found branch into working list: 1.1920928955078125e-06\n",
      "log prob nodes: \n",
      "1\n",
      ": \"в процессе\n",
      "\n",
      "get top k nodes call: 7.462501525878906e-05\n",
      "['в процессе']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1325838565826416"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "prompt = '''There is a request from the user to create a task in task management system. Please, extract the status of this task that will be placed in Task Management System.\n",
    "Example 1: Обнови статус задачи по проведению пентеста на выполнено.\n",
    "Answer: \"выполнено\"\n",
    "\n",
    "Example 2: \"Создай Ване задачу по рефакторингу кода. И поставь статус - в процессе\"\n",
    "Answer: \"в процессе\"\n",
    "\n",
    "Example 3: \"Покажи мне исполнителей задачи по построению графиков\"\n",
    "Answer: \"null\"\n",
    "\n",
    "Example 4: \"Создай задачу купить конфет, статус в процессе\"\n",
    "Answer: \"'''\n",
    "\n",
    "\n",
    "context = \"Создай задачу купить конфет, статус в процессе\"\n",
    "\n",
    "\n",
    "res = guidance_system.substring(prompt, context, 1)\n",
    "print(res)\n",
    "\n",
    "time.time() - s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b228c29a-4d7c-4772-bf6c-6befadc427bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BLOOM has ', 'BLOOM has 1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c327edf-56cf-4936-855f-f9e25e457bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: tensor([   30,   426,  1623,  1937,   706,   220, 10967,  7239,  5137]), -9.728892429207917\n",
      "? BLOOM has 176 billion parameters\n",
      "Nodes: tensor([   30,   426,  1623,  1937,   706,   220, 10967,  7239]), -9.645392401551362\n",
      "? BLOOM has 176 billion\n"
     ]
    }
   ],
   "source": [
    "for node in res[1]:\n",
    "    print(node)\n",
    "    print(guidance_system.tokenizer.decode(node.token_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c29f88d-257a-454f-aad5-6b0a8a8c66cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 86/86 [00:00<00:00, 697.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33488011360168457\n",
      "\n",
      "\n",
      "['good day!', 'good day! ', 'good ', 'ood day! ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"The weather is amazing! It is so \"\n",
    "context = \"good day! bad day, I hate it\"\n",
    "\n",
    "s = time.time()\n",
    "res = guidance_system.substring(prompt, context, 4)\n",
    "print(time.time() - s)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82b17fba-fd52-4aa9-bac6-44c76acd97a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What usually has 4 wheels? car', 'What usually has 4 wheels? horse', 'I have been in country in South Asia called Bangladesh', 'I have been in country in South Asia called Bangcheese']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00, 117.99it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m choices_list2 \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcar\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mladesh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheese\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m output1 \u001b[38;5;241m=\u001b[39m \u001b[43mguidance_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoices_list1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken for select() with choices_list1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end_time \u001b[38;5;241m-\u001b[39m start_time, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 107\u001b[0m, in \u001b[0;36mGuidanceBeta.select\u001b[0;34m(self, input_batches, choices_list, return_full_text)\u001b[0m\n\u001b[1;32m    105\u001b[0m log_prob_sequence \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(skip_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(outputs[candidate_idx]\u001b[38;5;241m.\u001b[39mprompt_logprobs)):\n\u001b[0;32m--> 107\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids_slice\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidate_idx\u001b[49m\u001b[43m]\u001b[49m[token_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    108\u001b[0m     log_prob_sequence\u001b[38;5;241m.\u001b[39mappend(outputs[candidate_idx]\u001b[38;5;241m.\u001b[39mprompt_logprobs[token_idx][token])\n\u001b[1;32m    110\u001b[0m log_prob_of_candidate \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mtensor(log_prob_sequence), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "texts = [\"What usually has 4 wheels? \", \"I have been in country in South Asia called Bang\"]\n",
    "choices_list1 = [[\"car\", \"horse\"], [\"ladesh\", \"cheese\"]]\n",
    "choices_list2 = [[\"horse\", \"car\"], [\"ladesh\", \"cheese\"]]\n",
    "start_time = time.time()\n",
    "output1 = guidance_system.select(texts, choices_list1)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for select() with choices_list1:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "output2 = guidance_system.select(texts, choices_list2)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for select() with choices_list2:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "print(output1)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4fa946f-97d5-482b-b6c1-c9a42437ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for gen(): 0.9958069324493408 seconds\n",
      "['s. I have owned three BMWs in my life, and I would own another one if I']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "gen_res = guidance_system.gen(\"I am a big fan of BMW\", max_length=30, min_length=10)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for gen():\", end_time - start_time, \"seconds\")\n",
    "print(gen_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67246964-5459-45c5-b8c6-73d1ae20a9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = \"How many programming languages does BLOOM support? \"\n",
    "choices_list = [[\"text\", \"46 languages\"]]\n",
    "guidance_system.select(texts, choices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7d215d-f815-4744-8f30-9cb66ab1621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> I am a big fan of BMW's and have owned one for 15 years. I have had the pleasure of driving the new M3 and M5 and they are a blast. I am looking for a used M3 for sale, but I am having a hard time finding one that I like. I am looking for one that is less than 5 years old, and has a manual transmission. I have been to several dealerships, and have looked at several\n",
      "\n",
      "\n",
      "Time taken for gen(): 4.105570316314697 seconds\n",
      "[\"'s and have owned one for 15 years. I have had the pleasure of driving the new M3 and M5 and they are a blast. I am looking for a used M3 for sale, but I am having a hard time finding one that I like. I am looking for one that is less than 5 years old, and has a manual transmission. I have been to several dealerships, and have looked at several\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(guidance_system.tokenizer)\n",
    "\n",
    "start_time = time.time()\n",
    "gen_res = guidance_system.gen(\"I am a big fan of BMW\",\n",
    "                              streamer=streamer,\n",
    "                              stop_keywords=\"BMW\",\n",
    "                              max_length=100)\n",
    "end_time = time.time()\n",
    "print()\n",
    "print()\n",
    "print(\"Time taken for gen():\", end_time - start_time, \"seconds\")\n",
    "print(gen_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f667bad-1b95-40a2-ac4c-d6cccf62f77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
