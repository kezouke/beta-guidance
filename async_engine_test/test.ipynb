{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_TOKEN=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\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "access_token = \"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\"\n",
    "%env HF_TOKEN=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-29 10:36:27,975\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-29 10:36:28 config.py:664] Chunked prefill is enabled (EXPERIMENTAL).\n",
      "INFO 05-29 10:36:28 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='meta-llama/Meta-Llama-3-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[W CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-29 10:36:30 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 05-29 10:36:33 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 05-29 10:36:33 gpu_executor.py:83] # GPU blocks: 11881, # CPU blocks: 2048\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from vllm import LLM\n",
    "\n",
    "# Specify the primary GPU to use (GPU 0, which has more available memory)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Setting PyTorch environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=1,  # Consider adjusting if still facing issues\n",
    "    gpu_memory_utilization=0.5,\n",
    "    enforce_eager=True,\n",
    "    enable_chunked_prefill=True,\n",
    "    max_num_batched_tokens=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 20.82it/s, Generation Speed: 20.88 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 12805, 5304, 264, 892]\n",
      "[]\n",
      "tensor([ 8.6250,  5.4375,  1.1641,  ..., -3.9375, -3.9375, -3.9375],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "\n",
      "tensor(-0.5781, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CompletionOutput(index=0, text=',', token_ids=[11], cumulative_logprob=-0.5771685242652893, logprobs=None, finish_reason=length, stop_reason=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from vllm import SamplingParams\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "def process_token(prompt_tokens, token_ids, logits):\n",
    "    print(prompt_tokens)\n",
    "    print(token_ids)\n",
    "    print(logits)\n",
    "    print()\n",
    "    log_probs = F.log_softmax(logits, dim=0)\n",
    "    print(log_probs[11])\n",
    "    return logits\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=1, prompt_logprobs=1, logits_processors=[process_token])\n",
    "\n",
    "# Benchmarking vLLM\n",
    "start_time_vllm = time.time()\n",
    "vllm_output = llm.generate(prompt, sampling_params)\n",
    "end_time_vllm = time.time()\n",
    "vllm_output[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 32.29it/s, Generation Speed: 32.48 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 12805, 5304, 264, 892, 25]\n",
      "[]\n",
      "tensor([ -1.7656,  -2.8750,  -1.9219,  ..., -10.1875, -10.1875, -10.1875],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=2, prompt='Once upon a time:', prompt_token_ids=[128000, 12805, 5304, 264, 892, 25], prompt_logprobs=[None, {12805: Logprob(logprob=-8.654012680053711, rank=381, decoded_token='Once'), 14924: Logprob(logprob=-1.185262680053711, rank=1, decoded_token='Question')}, {5304: Logprob(logprob=-2.052717447280884, rank=3, decoded_token=' upon'), 499: Logprob(logprob=-1.8027174472808838, rank=1, decoded_token=' you')}, {264: Logprob(logprob=-0.012741114012897015, rank=1, decoded_token=' a')}, {892: Logprob(logprob=-0.03552006930112839, rank=1, decoded_token=' time')}, {25: Logprob(logprob=-6.5146684646606445, rank=26, decoded_token=':'), 11: Logprob(logprob=-0.5771685242652893, rank=1, decoded_token=',')}], outputs=[CompletionOutput(index=0, text=' bathroom', token_ids=[15197], cumulative_logprob=-12.297264099121094, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1716914570.5700548, last_token_time=1716914570.5700548, first_scheduled_time=1716914570.5758898, first_token_time=1716914570.605934, time_in_queue=0.005835056304931641, finished_time=1716914570.6063328), lora_request=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from vllm import SamplingParams\n",
    "\n",
    "\n",
    "prompt = \"Once upon a time:\"\n",
    "\n",
    "def process_token(prompt_tokens, token_ids, logits):\n",
    "    print(prompt_tokens)\n",
    "    print(token_ids)\n",
    "    print(logits)\n",
    "    print()\n",
    "    logits[1070] = -9999.999 \n",
    "    logits[574] = -9999.999\n",
    "    logits[263] = -9999.999\n",
    "    return logits\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=1, prompt_logprobs=1, logits_processors=[process_token])\n",
    "\n",
    "# Benchmarking vLLM\n",
    "start_time_vllm = time.time()\n",
    "vllm_output = llm.generate(prompt, sampling_params)\n",
    "end_time_vllm = time.time()\n",
    "vllm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
